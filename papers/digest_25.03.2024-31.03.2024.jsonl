{"title":"CodePlan: Repository-level Coding using LLMs and Planning","link_main":"https:\/\/arxiv.org\/abs\/2309.12499","date":"21.09.2023","type":"paper","topic":"Agents","hashtags":"repository-level coding","link_tweet":null,"link_code":"https:\/\/github.com\/microsoft\/codeplan","link_website":null,"link_other":{},"notes":"Microsoft, FSE'24","reviews":"A relatively old paper on repository-level code editing.\nThe authors embarked on a journey to employ LLM-based agents for this task.\n\nThey employ static analysis as the planner augmentation for the agent, which seems reasonable for code.\nGiven an initial instruction (e.g., someone changed the definition of one function) the method:\n1. performs an analysis to find code chunks that are affected by this change (e.g., external calls, inheritance, etc.). If there are no chunks that are affected, the method goes directly to the testing step (5).\n2. each of those chunks of code is added to the planning graph, it is connected to each node of the graph it depends on. Each new node is marked as pending.\n3. each node of the computational graph, that has no pending nodes it depends on is offered to an agent to be changed. The prompt contains the following information:\n    a. the chunk of code to change\n    b. the spatial context (other methods in the same class, other functions in the same file, etc)\n    c. the temporal context (what's the diff from the method start to the current state)\n    d. the causal context (what's caused this node to be added, 4. what changes were introduced to the nodes it depends on)\nthe method goes to the first step, to add new nodes, now when the new set of changes was introduced.\n5. (we come here only when there are no unaltered affected chunks of code). The resulting repository is passed to some testing engine, and authors employ compilation\/type-checking as quality assurance.\n6. If there are some errors during the testing, they are introduced as initial instructions for the new iteration of the method, and everything goes to the first stage again.\n\nThe authors assess their method from two points of view:\n1. How well does it find the proper code chunks to edit. For this they measure matching blocks (edited both by the real programmer, and by the method), missed blocks (edited by real programmer but not by the method), and the spurious blocks (edited by the method, but not by the programmer), effectively finding accuracy and recall for their method in terms of code chunks.\n2. How well does the code editing work. For this they measure editor distance between the predicted and the target code, and diffBLEU, which is, effectively, BLEU measured between the diff produced by the method and the diff produced by the programmer. The authors argue, that it helps to not overwhelm BLEU with lots of code that wasn't changed.","included_in_digest":"#2 (25.03.2024 - 31.03.2024)"}
{"title":"Can large language models explore in-context?","link_main":"https:\/\/arxiv.org\/abs\/2403.15371","date":"22.03.2024","type":"paper","topic":"Other","hashtags":null,"link_tweet":null,"link_code":null,"link_website":null,"link_other":{},"notes":"Microsoft","reviews":"The authors tried LLMs as the strategy engines to solve the Multi-Armed Bandits problem.\nThe problem is stated as follows (you can skip this if you know the multi-armed bandits): the agent has several options (arms) with their own associated probability of the reward. At each step, the agent activates one arm, which samples the reward (either 0 or 1) from the corresponding distribution. The aim of the agent is to maximize the received reward during the T steps. A good strategy, therefore, should balance exploration (you need to have some samples from each arm to define the one with the largest average) with exploitation (you need to activate the arm with the largest average reward as many times as possible).\n\nThe authors compared the LLMs to several well-known strategies (Thompson Sampling, greedy, Upper Confidence Bound) and concluded that LLMs (mostly) behave like a greedy strategy. To test how different prompting affects the models, the authors considered two different scenarios (presenting the task as click-a-button or which-ad-to-choose), task framing (either directly prompting the model to explore or not), types of history (raw or pre-calculated averages), output formats (single action, or probability distribution over actions), and planning strategies (direct answer, or chain-of-thought). And all of them, except GPT-4 + one specific combination, were greedy. Several exceptions were uniform (activating all arms randomly, without any exploitation).\n\nThe authors conclude that LLMs aren't good enough to replace the sampling strategies. External tools, e.g., summarizing history, were able to improve performance a bit.\n\nMy practical takeaway is:\nWe should not forget that the LLMs are:\n* Language models indeed, and therefore, we should seek to extract from them the bias they learned on the great scale data they consumed instead of expecting them to behave as optimal algorithms on synthetic debiased problems. I guess if the arms were not made equal in appearance, the results may have changed drastically.\n* Inductive in the sense that their next prediction can be pretty much defined by their previous predictions when they are added to the prompt.\n* Faulty calculators (In my recent experience, GPT-4 tried to generate division by zero error by dividing 10 by 2).\n\nMy thoughts on the paper formatting:\n* The authors first discuss several results and introduce the metrics they use in those discussions and plots. I was never a fan of the maxim that \"the figure should appear strictly after the text that refers to it\", but explaining the metrics one section later than using them is too much.\n* The paper is overflown with small findings that aren't working on the larger story of the paper. I think this is a good example of how it can damage your paper to lay out too many details simultaneously. I, personally think that it could be better to move additional findings, not working on the storyline, to an appendix.","included_in_digest":"#2 (25.03.2024 - 31.03.2024)"}
{"title":"Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models","link_main":"https:\/\/arxiv.org\/abs\/2403.12881v1","date":"19.03.2024","type":"paper","topic":"Other","hashtags":"fine-tuning","link_tweet":null,"link_code":"https:\/\/github.com\/InternLM\/Agent-FLAN","link_website":"https:\/\/internlm.github.io\/Agent-FLAN\/","link_other":{"dataset":"https:\/\/huggingface.co\/datasets\/internlm\/Agent-FLAN","model":"https:\/\/huggingface.co\/internlm\/Agent-FLAN-7b"},"notes":"InternLM","reviews":null,"included_in_digest":"#2 (25.03.2024 - 31.03.2024)"}
{"title":"DebugBench: Evaluating Debugging Capability of Large Language Models","link_main":"https:\/\/arxiv.org\/abs\/2401.04621","date":"11.01.2024","type":"paper","topic":"Benchmarks & Environments","hashtags":"debugging","link_tweet":null,"link_code":"https:\/\/github.com\/thunlp\/DebugBench","link_website":null,"link_other":{"dataset":"https:\/\/huggingface.co\/datasets\/Rtian\/DebugBench"},"notes":null,"reviews":null,"included_in_digest":"#2 (25.03.2024 - 31.03.2024)"}
{"title":"LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks","link_main":"https:\/\/arxiv.org\/abs\/2402.01817","date":"06.02.2024","type":"paper","topic":"Reasoning & Planning","hashtags":null,"link_tweet":null,"link_code":null,"link_website":null,"link_other":{},"notes":null,"reviews":null,"included_in_digest":"#2 (25.03.2024 - 31.03.2024)"}
{"title":"RepoHyper: Better Context Retrieval Is All You Need for Repository-Level Code Completion","link_main":"https:\/\/arxiv.org\/abs\/2403.06095","date":"16.03.2024","type":"paper","topic":"Other","hashtags":"repository-level coding","link_tweet":null,"link_code":"https:\/\/github.com\/FSoft-AI4Code\/RepoHyper","link_website":null,"link_other":{},"notes":null,"reviews":null,"included_in_digest":"#2 (25.03.2024 - 31.03.2024)"}
{"title":"R2E: Turning any Github Repository into a Programming Agent Test Environment","link_main":"https:\/\/openreview.net\/pdf?id=xsytkViOsd","date":"11.03.2024","type":"paper","topic":"Benchmarks & Environments","hashtags":"repository-level coding","link_tweet":null,"link_code":null,"link_website":null,"link_other":{},"notes":"LLMAgents @ ICLR'24","reviews":null,"included_in_digest":"#2 (25.03.2024 - 31.03.2024)"}
{"title":"Enhancing LLM-Based Coding Tools through Native Integration of IDE-Derived Static Context","link_main":"https:\/\/arxiv.org\/abs\/2402.03630","date":"19.02.2024","type":"paper","topic":"Other","hashtags":null,"link_tweet":null,"link_code":null,"link_website":null,"link_other":{},"notes":"LLM4Code @ ICSE'24","reviews":null,"included_in_digest":"#2 (25.03.2024 - 31.03.2024)"}
{"title":"MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution","link_main":"https:\/\/arxiv.org\/abs\/2403.17927","date":"26.03.2024","type":"paper","topic":"Agents","hashtags":"repository-level coding","link_tweet":null,"link_code":null,"link_website":null,"link_other":{},"notes":null,"reviews":null,"included_in_digest":"#2 (25.03.2024 - 31.03.2024)"}
{"title":"MacGyver: Are Large Language Models Creative Problem Solvers?","link_main":"https:\/\/arxiv.org\/abs\/2311.09682","date":"27.03.2024","type":"paper","topic":"Benchmarks & Environments","hashtags":null,"link_tweet":null,"link_code":"https:\/\/github.com\/allenai\/MacGyver","link_website":null,"link_other":{},"notes":"NAACL'24, AI2","reviews":null,"included_in_digest":"#2 (25.03.2024 - 31.03.2024)"}
{"title":"Evaluating Large Language Models with Runtime Behavior of Program Execution","link_main":"https:\/\/arxiv.org\/abs\/2403.16437","date":"25.03.2024","type":"paper","topic":"Other","hashtags":"debugging","link_tweet":null,"link_code":null,"link_website":null,"link_other":{},"notes":null,"reviews":null,"included_in_digest":"#2 (25.03.2024 - 31.03.2024)"}
